# Hidden Markov Model (HMM) for DS18B20 Sensor State Detection
# =========================================================

# This notebook demonstrates how to use Hidden Markov Models to detect
# when the DS18B20 temperature sensor is connected or disconnected.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from influxdb_client import InfluxDBClient
from hmmlearn import hmm
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import joblib
import warnings
warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output

# Set up plotting style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("notebook", font_scale=1.2)

# Set Matplotlib parameters for better readability
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['font.size'] = 12

# 1. Introduction to Hidden Markov Models
# ======================================
print("""
# Hidden Markov Models Explained
# =============================

Hidden Markov Models (HMMs) are statistical models ideal for representing systems 
that transition between discrete states where:

1. The actual state is hidden (we can't directly observe it)
2. We only observe outputs generated by those hidden states

In our case:
- Hidden States: "Connected" and "Disconnected" 
- Observations: Temperature measurements from DS18B20, Si7021, and RPi CPU

## Key Components of an HMM:

1. **States**: The set of hidden states (Connected/Disconnected)

2. **Transition Probability Matrix**: The probability of transitioning from one state to another
   - P(Disconnected → Connected)
   - P(Connected → Disconnected)
   - P(Staying in current state)

3. **Emission Probabilities**: The probability of observing particular measurements given each state
   - When Connected: DS18B20 likely reads 97-99°F
   - When Disconnected: DS18B20 likely reads closer to room temperature

4. **Initial State Distribution**: The probability of starting in each state

## What makes HMMs powerful for sensor state detection:

1. They handle **uncertainty** in both state transitions and measurements
2. They consider the **temporal sequence** of observations
3. They can **learn patterns** from labeled or unlabeled data
4. They provide **probabilistic outputs** (confidence in the state estimation)

Let's implement an HMM to detect when the DS18B20 sensor is connected/disconnected!
""")

# 2. Connect to InfluxDB and Query Data
# ====================================
# Replace these with your actual InfluxDB credentials
url = "https://your-influxdb-url"
token = "your-generated-token"
org = "your-organization"
bucket = "your-bucket"

print("\nConnecting to InfluxDB...")
client = InfluxDBClient(url=url, token=token, org=org)
query_api = client.query_api()

# Define the time range for data retrieval (e.g., last 7 days)
time_range = "7d"

print(f"Retrieving data from the last {time_range}...")

# Query for DS18B20 temperature data
ds18b20_query = f'''
    from(bucket: "{bucket}")
        |> range(start: -{time_range})
        |> filter(fn: (r) => r["_measurement"] == "DS18B20")
        |> filter(fn: (r) => r["dimension"] == "temperature")
        |> filter(fn: (r) => r["_field"] == "value")
        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)
'''

# Query for Si7021 temperature data (room temperature)
si7021_query = f'''
    from(bucket: "{bucket}")
        |> range(start: -{time_range})
        |> filter(fn: (r) => r["_measurement"] == "SI7021")
        |> filter(fn: (r) => r["dimension"] == "temperature")
        |> filter(fn: (r) => r["_field"] == "value")
        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)
'''

# Query for RPi CPU temperature
rpi_query = f'''
    from(bucket: "{bucket}")
        |> range(start: -{time_range})
        |> filter(fn: (r) => r["_measurement"] == "RPi Zero 2W")
        |> filter(fn: (r) => r["dimension"] == "temperature")
        |> filter(fn: (r) => r["_field"] == "value")
        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)
'''

# Execute queries and convert to DataFrame
ds18b20_result = query_api.query_data_frame(ds18b20_query)
si7021_result = query_api.query_data_frame(si7021_query)
rpi_result = query_api.query_data_frame(rpi_query)

# Check if we have data
if ds18b20_result.empty or si7021_result.empty or rpi_result.empty:
    print("One or more queries returned no data. Please check your InfluxDB queries.")
else:
    print(f"Retrieved {len(ds18b20_result)} data points for DS18B20")
    print(f"Retrieved {len(si7021_result)} data points for Si7021")
    print(f"Retrieved {len(rpi_result)} data points for RPi CPU")

# 3. Process and Merge Data
# =========================
# Keep only necessary columns and rename for clarity
ds18b20_df = ds18b20_result[['_time', '_value']].rename(columns={'_value': 'core'})
si7021_df = si7021_result[['_time', '_value']].rename(columns={'_value': 'room'})
rpi_df = rpi_result[['_time', '_value']].rename(columns={'_value': 'cpu'})

# Make time the index for easier merging
ds18b20_df.set_index('_time', inplace=True)
si7021_df.set_index('_time', inplace=True)
rpi_df.set_index('_time', inplace=True)

# Resample data to ensure consistent timestamps (5-minute intervals)
ds18b20_df = ds18b20_df.resample('5T').mean()
si7021_df = si7021_df.resample('5T').mean()
rpi_df = rpi_df.resample('5T').mean()

# Merge all data into a single DataFrame
df = pd.concat([ds18b20_df, si7021_df, rpi_df], axis=1)

# Handle missing values (if any)
# Here we use forward fill followed by backward fill for any remaining NaNs
df = df.fillna(method='ffill').fillna(method='bfill')

print("\nMerged data sample:")
print(df.head())
print("\nDataset information:")
print(df.info())
print("\nSummary statistics:")
print(df.describe())

# 4. Exploratory Data Analysis
# ============================
print("\nPerforming exploratory data analysis...")

plt.figure(figsize=(14, 10))

# Plot temperature data
plt.subplot(3, 1, 1)
plt.plot(df.index, df['core'], 'b-', label='DS18B20')
plt.title('DS18B20 Temperature Over Time')
plt.ylabel('Temperature (°F)')
plt.legend()

plt.subplot(3, 1, 2)
plt.plot(df.index, df['room'], 'g-', label='Si7021 (Room)')
plt.title('Room Temperature Over Time')
plt.ylabel('Temperature (°F)')
plt.legend()

plt.subplot(3, 1, 3)
plt.plot(df.index, df['cpu'], 'r-', label='RPi CPU')
plt.title('CPU Temperature Over Time')
plt.ylabel('Temperature (°F)')
plt.legend()

plt.tight_layout()
plt.savefig('hmm_temperature_time_series.png')
plt.show()

# Histogram of DS18B20 temperatures
plt.figure(figsize=(14, 6))
sns.histplot(df['core'], bins=50, kde=True)
plt.title('Distribution of DS18B20 Temperature Readings')
plt.xlabel('Temperature (°F)')
plt.ylabel('Frequency')
plt.savefig('ds18b20_histogram.png')
plt.show()

# Let's check if we can see bimodal distribution (connected vs disconnected)
# Bimodal distribution would suggest two distinct states
print("\nChecking for bimodal distribution in DS18B20 temperatures...")

# 5. Preparing for HMM
# ===================
print("\nPreparing data for Hidden Markov Model...")

# 5.1 Creating feature vectors
# For HMM, we'll use both the absolute temperatures and the differences
# between sensors to capture relationships

# Calculate temperature differences
df['ds18b20_room_diff'] = df['core'] - df['room']
df['ds18b20_cpu_diff'] = df['core'] - df['cpu']
df['room_cpu_diff'] = df['room'] - df['cpu']

# Create feature matrix
feature_cols = ['core', 'room', 'cpu', 
                'ds18b20_room_diff', 'ds18b20_cpu_diff']
X = df[feature_cols].values

# Standardize features (important for HMM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a DataFrame of scaled features for easier reference
scaled_df = pd.DataFrame(X_scaled, index=df.index, columns=feature_cols)

# Visualize the scaled features
plt.figure(figsize=(14, 8))
for col in scaled_df.columns:
    plt.plot(scaled_df.index, scaled_df[col], label=col)
plt.title('Standardized Features')
plt.legend()
plt.savefig('hmm_standardized_features.png')
plt.show()

# 5.2 Initial state labeling for supervised training
# We'll use a temperature threshold to create an initial labeling
# This will help us initialize the HMM parameters

# Assuming body temperature is typically above 90°F
body_temp_threshold = 90.0
df['initial_label'] = (df['core'] > body_temp_threshold).astype(int)

# Visualize initial labeling
plt.figure(figsize=(14, 6))
plt.plot(df.index, df['core'], 'b-', label='DS18B20 Temp')
plt.plot(df.index, df['room'], 'g-', alpha=0.5, label='Room Temp')

# Highlight regions based on initial labels
connected_regions = df[df['initial_label'] == 1]
for idx, row in connected_regions.iterrows():
    plt.axvline(x=idx, color='r', alpha=0.2)

plt.axhline(y=body_temp_threshold, color='r', linestyle='--', label=f'Threshold ({body_temp_threshold}°F)')
plt.title('Initial State Labeling for HMM')
plt.ylabel('Temperature (°F)')
plt.legend()
plt.savefig('hmm_initial_labeling.png')
plt.show()

print(f"\nInitial labeling: {df['initial_label'].value_counts().to_dict()}")
print(f"  State 0 (Disconnected): {df['initial_label'].value_counts().get(0, 0)} data points")
print(f"  State 1 (Connected): {df['initial_label'].value_counts().get(1, 0)} data points")

# 6. Building the Hidden Markov Model
# ==================================
print("\nBuilding and training the Hidden Markov Model...")

# EXPLANATION: Hidden Markov Model Components
# =========================================
print("""
# Hidden Markov Model Components for Sensor State Detection
# ========================================================

We're building a 2-state HMM where:

1. The hidden states represent:
   - State 0: DS18B20 sensor is DISCONNECTED
   - State 1: DS18B20 sensor is CONNECTED to a body

2. The observation sequences are our features:
   - DS18B20 temperature
   - Room temperature
   - CPU temperature
   - Temperature differences

The HMM will learn:
- How likely the sensor is to transition between states
- What temperature patterns are typical in each state
- The probability distribution of observations for each state

We'll use the GaussianHMM from hmmlearn, which models emissions as 
multivariate Gaussian distributions (normal distributions in multiple dimensions).
""")

# 6.1 Initialize and train the HMM
# We'll use a Gaussian HMM with 2 states (connected/disconnected)
n_states = 2  # Connected and Disconnected states

# Create the HMM model
model = hmm.GaussianHMM(
    n_components=n_states,
    covariance_type="full",  # Full covariance matrix for flexibility
    n_iter=100,              # Maximum number of iterations
    random_state=42,         # For reproducibility
    verbose=True
)

# Initialize parameters based on our initial labeling
# This helps the model converge to a meaningful solution

# Start probabilities: proportion of each state in initial labels
start_prob = np.zeros(n_states)
for i in range(n_states):
    start_prob[i] = (df['initial_label'] == i).mean()
model.startprob_ = start_prob

# Transition probabilities: estimated from labeled sequences
# We'll calculate how often it transitions from one state to another
trans_mat = np.zeros((n_states, n_states))
for i in range(len(df) - 1):
    from_state = df['initial_label'].iloc[i]
    to_state = df['initial_label'].iloc[i + 1]
    trans_mat[from_state, to_state] += 1

# Normalize rows to get probabilities
for i in range(n_states):
    row_sum = trans_mat[i].sum()
    if row_sum > 0:
        trans_mat[i] = trans_mat[i] / row_sum
model.transmat_ = trans_mat

# Initialize means and covariances
means = np.zeros((n_states, X_scaled.shape[1]))
covars = np.zeros((n_states, X_scaled.shape[1], X_scaled.shape[1]))

for i in range(n_states):
    state_data = X_scaled[df['initial_label'] == i]
    if len(state_data) > 0:
        means[i] = np.mean(state_data, axis=0)
        covars[i] = np.cov(state_data, rowvar=False)
    else:
        # Fallback if no data for a state
        means[i] = np.zeros(X_scaled.shape[1])
        covars[i] = np.eye(X_scaled.shape[1])

model.means_ = means
model.covars_ = covars

# Print initial parameters
print("\nInitial HMM Parameters:")
print(f"Start probabilities: {model.startprob_}")
print(f"Transition matrix:\n{model.transmat_}")
print(f"Means:\n{model.means_}")

# 6.2 Fit the model to the data
# Now we'll let the EM algorithm refine our initial parameters
try:
    model.fit(X_scaled)
    print("\nHMM training successful!")
    print(f"Log likelihood: {model.score(X_scaled):.2f}")
    print(f"Number of iterations: {model.monitor_.iter}")
    
    # Print learned parameters
    print("\nLearned HMM Parameters:")
    print(f"Start probabilities: {model.startprob_}")
    print(f"Transition matrix:\n{model.transmat_}")
    print(f"Means:\n{model.means_}")
    
    # 7. Analyzing the HMM Results
    # ===========================
    print("\nAnalyzing HMM results...")
    
    # 7.1 Decode the most likely sequence of states
    hidden_states = model.predict(X_scaled)
    df['hmm_state'] = hidden_states
    
    # Adjust states if necessary to match our expectations
    # Sometimes the HMM might flip the states (0=connected, 1=disconnected)
    # We'll check if state 1 has higher DS18B20 temps on average
    state0_mean = df[df['hmm_state'] == 0]['core'].mean()
    state1_mean = df[df['hmm_state'] == 1]['core'].mean()
    
    # If state 0 has higher temps, it should be the "connected" state - flip the labels
    if state0_mean > state1_mean:
        print("\nFlipping state labels to match expectations (0=disconnected, 1=connected)")
        df['hmm_state'] = 1 - df['hmm_state']
    
    # 7.2 Calculate state probabilities
    # The forward-backward algorithm gives us probabilities for each state at each time
    state_probs = model.predict_proba(X_scaled)
    
    # If we flipped the states, also flip the probabilities
    if state0_mean > state1_mean:
        state_probs = 1 - state_probs
    
    # Add probabilities to DataFrame
    df['connected_prob'] = state_probs[:, 1]  # Probability of being in "connected" state
    
    # 7.3 Visualize the results
    plt.figure(figsize=(14, 12))
    
    # Plot 1: Original temperature data
    plt.subplot(3, 1, 1)
    plt.plot(df.index, df['core'], 'b-', label='DS18B20')
    plt.plot(df.index, df['room'], 'g-', alpha=0.5, label='Room (Si7021)')
    plt.title('Temperature Data')
    plt.ylabel('Temperature (°F)')
    plt.legend()
    
    # Plot 2: HMM-decoded states
    plt.subplot(3, 1, 2)
    for state in [0, 1]:
        mask = df['hmm_state'] == state
        label = 'Connected' if state == 1 else 'Disconnected'
        color = 'red' if state == 1 else 'blue'
        plt.plot(df.index[mask], df['core'][mask], 
                 'o', markersize=4, alpha=0.7, color=color, label=label)
    plt.title('HMM-Detected Sensor States')
    plt.ylabel('Temperature (°F)')
    plt.legend()
    
    # Plot 3: Connection probability
    plt.subplot(3, 1, 3)
    plt.plot(df.index, df['connected_prob'], 'k-')
    plt.fill_between(df.index, 0, df['connected_prob'], alpha=0.3, color='orange')
    plt.axhline(y=0.5, color='r', linestyle='--', label='Decision Threshold (0.5)')
    plt.title('Probability of DS18B20 Being Connected')
    plt.ylabel('Probability')
    plt.ylim(0, 1)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('hmm_results.png')
    plt.show()
    
    # 7.4 Analyze state transitions
    # Let's find where the sensor changes state
    df['state_change'] = df['hmm_state'].diff().abs() > 0
    transitions = df[df['state_change']]
    
    print(f"\nDetected {len(transitions)} state transitions:")
    if len(transitions) > 0:
        for i, (idx, row) in enumerate(transitions.iterrows()):
            state = "connected" if row['hmm_state'] == 1 else "disconnected"
            print(f"  {i+1}. {idx.strftime('%Y-%m-%d %H:%M:%S')}: Sensor became {state}")
    
    # 7.5 Compare with threshold-based approach
    agreement = (df['hmm_state'] == df['initial_label']).mean() * 100
    print(f"\nAgreement between threshold-based and HMM detection: {agreement:.2f}%")
    
    # Confusion matrix-like comparison
    confusion = pd.crosstab(
        df['initial_label'], 
        df['hmm_state'],
        rownames=['Threshold (90°F)'],
        colnames=['HMM']
    )
    
    print("\nComparison of detection methods:")
    print(confusion)
    
    # 7.6 Analyze observation distributions
    print("\nAnalyzing temperature distributions by detected state...")
    
    # Temperature statistics by state
    state_stats = df.groupby('hmm_state')['core'].agg(['mean', 'std', 'min', 'max'])
    print("\nDS18B20 temperature statistics by state:")
    print(state_stats)
    
    # Visualize temperature distributions
    plt.figure(figsize=(14, 6))
    for state in [0, 1]:
        label = 'Connected' if state == 1 else 'Disconnected'
        data = df[df['hmm_state'] == state]['core']
        sns.kdeplot(data, label=label, fill=True, alpha=0.5)
    
    plt.title('DS18B20 Temperature Distribution by State')
    plt.xlabel('Temperature (°F)')
    plt.ylabel('Density')
    plt.legend()
    plt.savefig('hmm_temperature_distributions.png')
    plt.show()
    
    # 8. Creating a Reusable Detection Function
    # =======================================
    print("\nCreating a reusable detection function...")
    
    def detect_connection_hmm(core, room, cpu, model=model, scaler=scaler):
        """
        Detect if the DS18B20 sensor is connected using the HMM model.
        
        Parameters:
        -----------
        core : float
            Current DS18B20 temperature reading (°F)
        room : float
            Current room temperature from Si7021 (°F)
        cpu : float
            Current CPU temperature (°F)
        model : hmmlearn.hmm.GaussianHMM
            Trained HMM model
        scaler : sklearn.preprocessing.StandardScaler
            Fitted scaler for feature standardization
            
        Returns:
        --------
        dict
            Dictionary containing detection results and diagnostics
        """
        # Calculate derived features
        ds18b20_room_diff = core - room
        ds18b20_cpu_diff = core - cpu
        
        # Create feature vector
        X = np.array([[core, room, cpu, 
                       ds18b20_room_diff, ds18b20_cpu_diff]])
        
        # Standardize features
        X_scaled = scaler.transform(X)
        
        # Get state prediction and probability
        state = model.predict(X_scaled)[0]
        state_probs = model.predict_proba(X_scaled)[0]
        
        # Adjust if necessary (if states were flipped)
        if state0_mean > state1_mean:
            state = 1 - state
            state_probs = state_probs[::-1]
        
        # Create result
        result = {
            'core': core,
            'room': room,
            'cpu': cpu,
            'is_connected': state == 1,
            'connected_probability': state_probs[1],
            'disconnected_probability': state_probs[0]
        }
        
        return result
    
    # Test the function with the last data point
    last_point = df.iloc[-1]
    detection_result = detect_connection_hmm(
        last_point['core'],
        last_point['room'],
        last_point['cpu'],
        model=model,
        scaler=scaler
    )
    
    print("\nDetection result for the most recent data point:")
    for key, value in detection_result.items():
        print(f"{key}: {value}")
    
    # 9. Save the Model for Later Use
    # =============================
    # Save the HMM model and scaler
    joblib.dump({
        'model': model,
        'scaler': scaler,
        'state0_mean': state0_mean,
        'state1_mean': state1_mean
    }, 'ds18b20_hmm_model.pkl')
    
    print("\nSaved model and parameters to 'ds18b20_hmm_model.pkl'")
    
    # 10. Final Recommendations
    # =======================
    print("""
    # Summary and Recommendations
    # =========================
    
    This notebook demonstrates how to use Hidden Markov Models to detect when the DS18B20 
    temperature sensor is connected to a human body versus when it's disconnected and 
    measuring ambient temperature.
    
    Key findings:
    1. HMMs can effectively capture the hidden connection state from temperature readings
    2. The model learns the probability distributions for each state and transitions between them
    3. We get a confidence score (probability) rather than just a binary decision
    
    Advantages of the HMM approach:
    1. More robust to noise and temperature fluctuations
    2. Considers temporal patterns and state transitions
    3. Provides probabilistic output for better decision-making
    4. Less reliant on fixed thresholds
    
    Recommendations for implementation:
    1. Retrain the model periodically as more data becomes available
    2. Consider using a sliding window approach for online detection
    3. Combine with the residual analysis method for higher confidence
    4. Implement a "stabilization" phase after detected transitions
    
    Next steps:
    1. Evaluate with ground truth data where connection status is known
    2. Consider a more complex model with more states (e.g., disconnected, 
       stabilizing, connected, mispositioned)
    3. Explore other features like temperature derivatives
    """)

except Exception as e:
    print(f"Error training HMM: {e}")
    print("Please check your data or try a different initialization strategy.")

# Close the InfluxDB client
client.close()
print("\nAnalysis complete!")